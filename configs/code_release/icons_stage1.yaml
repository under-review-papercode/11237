model_params:
  name: "SVG_VAQVAE"  # only model available for this submission
  vector_decoder_model: "mlp" # "mlp" or "raster_conv"
  quantized_dim: 512
  codebook_size: 4096  # will be ignored for FSQ
  image_loss: "mse"  # "pyramid" or "mse"
  single_code_representation: true
  vq_method: "fsq"  # "vqvae", "FSQ", "vqtorch"
  fsq_levels: [7,5,5,5,5]  # will determine codebook_size, see Table 1 of FSQ paper - [7,5,5,5,5] for 4096 (e.g. StrokeNUWA), [8,5,5,5] for 1024
  num_segments: 2
  # geometric_constraint: "inner_distance"
  # geometric_constraint_weight: 0.1

data_params:
  dataset: "stage1"  # must be in the DATASETMAP in run.py
  image_root_dir: ""  # TODO: path to the svg_simplified downloaded from HF
  csv_path: ""  # TODO: absolute path to the split.csv inside the svg_simplified folder
  channels: 3  # use 3
  width: 128
  train_batch_size: 8
  val_batch_size:  8
  num_workers: 0
  individual_max_length : 7.5  # max length of a single stroke, decides the width/height of rendered shapes
  stroke_width: 0.7  # this must scale with individiual_max_length as they together define the image
  # max_shapes_per_svg: 32

exp_params:
  lr: 0.00002  # 0.00002
  scheduler_type: step # cosine, exponential, none, step
  weight_decay: 1.e-4  # specify positive float to enable, start experimenting with 1.e-4/1.e-3
  step_lr_epoch_step_size: 3000
  scheduler_gamma: 0.98  # 0.95 is a good starting value
  train_log_interval: 0.1
  manual_seed: 1265
  min_lr: 0.0000001

trainer_params:
  devices: -1  # always keep at -1 as this takes all available GPUs specified through CUDA_VISIBLE_DEVICES
  max_epochs: 250  # dsnt matter too much, got early stopping implemented
  val_check_interval: 0.03  # per step validation
  limit_val_batches: 0.0005
  # accumulate_grad_batches: 2

logging_params:
  entity: ""  # TODO: use your wandb entity if you want to log
  project: ""  # TODO: your wandb project name
  save_dir: ""  # TODO: lighting dump folder for weights, logs and config
  name: ""  # TODO: name of your exp if you use wandb
  version: 0
  author: ""  # TODO: will be a tag in wandb
  allow_val_change: False  # allow changing values in this config w.r.t. the run that you're continuing (good for changing loss weightings mid-run)