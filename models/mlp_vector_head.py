import torch
from torch import Tensor
from torch import nn
from torch.nn import functional as F
from typing import List
import pydiffvg
import wandb
    
class MLPVectorHeadFixed(nn.Module):
    """
    The MLP vector head is the simplest possible head for the VSQ. 
    It takes the latent vector generated by the transformer and predicts the coordinates of cubic Bezier curves directly through a linear layer.
    
    Constructor Args:
        - latent_dim: int, the dimensionality of the latent vector input
        - segments: int, the number of segments that make a single path, default 4
        - imsize: int, the size of the rasterized image, default 128
        - color_output: bool, whether to predict the color of the stroke, default False
        - alpha_prediction: bool, whether to predict the alpha of the stroke, default False
        - stroke_width_predictor: bool, whether to predict the stroke width, default True
        - max_stroke_width: float, the maximum width of the stroke of the cubic Bezier curves, default 10.0

    Some code snippits taken from: 
        - https://github.com/BachiLi/diffvg/blob/master/apps/generative_models/models.py#L17
        - https://github.com/BachiLi/diffvg/blob/master/apps/generative_models/rendering.py
        - https://github.com/BachiLi/diffvg/blob/master/apps/painterly_rendering.py

    """
    def __init__(self, 
                 latent_dim=128, 
                 segments: int = 4, 
                 imsize=128,
                 color_output=False,
                 alpha_prediction = False,
                 stroke_width_predictor: bool = True,
                 max_stroke_width: float = 10.0):
        super(MLPVectorHeadFixed, self).__init__()

        self.stroke_width = max_stroke_width
        self.min_stroke_width = 0.3
        self.imsize = imsize
        self.segments = segments
        self.latent_dim = latent_dim
        self.stroke_width_predictor = stroke_width_predictor

        # 4 points bezier with n_segments -> 3*n_segments + 1 points
        self.point_predictor = nn.Sequential(
            nn.Linear(self.latent_dim, self.latent_dim),
            nn.SELU(),
            nn.Linear(self.latent_dim, self.latent_dim),
            nn.SELU(),
            nn.Linear(self.latent_dim, 2 * (self.segments * 3 + 1)),
            nn.Sigmoid()  # bound spatial extent
        )

        self.stroke_predictor = nn.Sequential(
            nn.Linear(self.latent_dim, self.latent_dim),
            nn.SELU(),
            nn.Linear(self.latent_dim, 1, bias=False),
            nn.Sigmoid()
        )

        self.alpha_predictor = None
        if alpha_prediction:
            self.alpha_predictor = nn.Sequential(
                nn.Linear(self.latent_dim, 1, bias=False),
                nn.Sigmoid()
            )

        self.color_predictor = None
        if color_output:
            self.color_predictor = nn.Sequential(
                nn.Linear(self.latent_dim, 3, bias=False),
                nn.Sigmoid()
            )

    def forward(self, z, primitive: str = "cubic", **kwargs):
        logging_dict = {}
        bs = z.shape[0]

        feats = z
        all_points = self.point_predictor(feats)

        all_width = self.stroke_predictor(feats)
        if self.stroke_width_predictor:
            all_widths = self.stroke_predictor(feats) * self.stroke_width
        else:
            all_widths = torch.ones_like(all_width) * 4.7

        all_widths = torch.max(all_widths, torch.ones_like(all_widths)*self.min_stroke_width)  # enforce min stroke width

        logging_dict["mean_stroke_width"] = all_widths.detach().mean()

        if self.color_predictor:
            all_colors = self.color_predictor(feats)
            all_colors = all_colors.view(bs, 1, 3)
        else:
            all_colors = None
        
        if self.alpha_predictor:
            all_alphas = self.alpha_predictor(feats)
        else:
            all_alphas = torch.ones(all_widths.shape, device=all_widths.device)

        all_points = all_points.view(bs, 1, self.segments * 3 + 1, 2)

        output, scenes = self.bezier_render(
            all_points, all_widths, all_alphas,
            colors=all_colors,
            canvas_size=self.imsize,
            primitive=primitive
        )

        return [output, scenes, all_points, all_widths], logging_dict
    def render(self,
                canvas_width, 
                canvas_height, 
                shapes, 
                shape_groups, 
                samples=2,
                seed=42):
        
        _render = pydiffvg.RenderFunction.apply
        scene_args = pydiffvg.RenderFunction.serialize_scene(
            canvas_width, canvas_height, shapes, shape_groups)
        img = _render(canvas_width, canvas_height, samples, samples,
                    seed,   # seed
                    None,  # background image
                    *scene_args)
        return img

    def bezier_render(self, all_points: Tensor, all_widths: Tensor, all_alphas: Tensor,
                    canvas_size=32, primitive: str = "cubic", colors=None, white_background=True):
        device = all_points.device

        # all_points = 0.5*(all_points + 1.0) * canvas_size
        all_points = all_points * canvas_size

        eps = 1e-4
        all_points = all_points + eps*torch.randn_like(all_points, device=device)

        bs, num_strokes, num_pts, _ = all_points.shape
        num_segments = (num_pts - 1) // 3
        n_out = 4
        output = torch.zeros(bs, n_out, canvas_size, canvas_size,
                        device=device)

        scenes = []
        for batch in range(bs):
            shapes = []
            shape_groups = []
            for p in range(num_strokes):
                points = all_points[batch, p].contiguous()  # (num_pts, 2)
                if primitive == "cubic":
                    num_ctrl_pts = torch.zeros(num_segments, dtype=torch.int32) + 2
                elif primitive == "linear":
                    if num_segments > 1:
                        raise NotImplementedError("Linear primitive only supports 1 segment atm")
                    num_ctrl_pts = torch.zeros(num_segments, dtype=torch.int32)
                    points = points[[0, 3]]
                elif primitive == "quadratic":
                    num_ctrl_pts = torch.zeros(num_segments, dtype=torch.int32) + 1
                    points = points[[0, 1, 3]]
                else:
                    raise NotImplementedError(f"Primitive {primitive} not implemented")
                width = all_widths[batch, p]
                alpha = all_alphas[batch, p]
                if colors is not None:
                    color = colors[batch, p]
                else:
                    color = torch.zeros(3, device=device)

                color = torch.cat([color, alpha.view(1,)])

                path = pydiffvg.Path(
                    num_control_points=num_ctrl_pts, points=points,
                    stroke_width=width, is_closed=False)
                shapes.append(path)
                path_group = pydiffvg.ShapeGroup(
                    shape_ids=torch.tensor([len(shapes) - 1]),
                    fill_color=None,
                    stroke_color=color)
                shape_groups.append(path_group)

            # Rasterize
            scenes.append((canvas_size, canvas_size, shapes, shape_groups))
            raster = self.render(canvas_size, canvas_size, shapes, shape_groups,
                            samples=2)
            raster = raster.permute(2, 0, 1).view(4, canvas_size, canvas_size)

            # alpha = raster[3:4]
            # if colors is not None:  # color output
            #     image = raster[:3]
            #     alpha = alpha.repeat(3, 1, 1)
            # else:
            #     image = raster[:1]

            # # alpha compositing
            # image = image*alpha
            # output[k] = image
            output[batch] = raster

        output = output.to(device)
        
        if white_background:
            alpha = output[:, 3:4, :, :]
            output_white_bg = output[:, :3, :, :] * alpha + (1 - alpha)
            output = torch.cat([output_white_bg, alpha], dim=1)

        return output, scenes

class MLPRasterHead(nn.Module):
    """
    The MLPRasterHead is can be used for debugging the VSQ as it does not have the limitations of DIffVG. 
    It takes the latent vector generated by the transformer and predicts the raster image directly through a linear layer without any SVG stuff.
    Limitation: only B&W images are supported.
    """
    def __init__(self,
                 latent_dim: int = 128,
                 render_size: int = 128,
                 **kwargs) -> None:
        super().__init__(**kwargs)

        self.latent_dim = latent_dim
        self.render_size = render_size

        self.vector_decoder = nn.Sequential(
                nn.Linear(latent_dim, 256),
                nn.ReLU(),
                nn.Linear(256, 512),
                nn.ReLU(),
                nn.Linear(512, 1024),
                nn.ReLU(),
                nn.Linear(1024, 2048),
                nn.ReLU(),
                nn.Linear(2048, 4096),
                nn.ReLU(),
                nn.Linear(4096, 4096*2),
                nn.ReLU(),
                nn.Linear(4096*2, 1 * render_size * render_size),
                nn.Sigmoid()
            )
        
    def forward(self, z: Tensor, **kwargs) -> Tensor:
        """
        z is a single timestep of shape (bs, latent_dim)
        """
        raster_images = self.vector_decoder(z)
        raster_images = raster_images.view(-1, 1, self.render_size, self.render_size)
        raster_images = raster_images.repeat(1, 3, 1, 1)
        return [raster_images]
